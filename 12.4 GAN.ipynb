{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fea086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Reshape, Flatten, LeakyReLU, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR) \n",
    "\n",
    "# Parametry\n",
    "latent_dim = 100\n",
    "img_shape = (28, 28, 1)\n",
    "\n",
    "# Tworzenie generatora\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    return model\n",
    "\n",
    "# Tworzenie dyskryminatora\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Kompilacja modeli\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "z = tf.keras.Input(shape=(latent_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "gan = tf.keras.Model(z, valid)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Wczytanie danych MNIST\n",
    "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 127.5 - 1.0  # Normalizacja do zakresu [-1, 1]\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "\n",
    "# Trenowanie GAN\n",
    "def train(epochs, batch_size=128, save_interval=1000):\n",
    "    half_batch = batch_size // 2\n",
    "    for epoch in range(epochs):\n",
    "        # Trening dyskryminatora\n",
    "        idx = np.random.randint(0, x_train.shape[0], half_batch)\n",
    "        imgs = x_train[idx]\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Trening generatora\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_y = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "        \n",
    "        # Wyświetlanie wyników\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
    "            sample_images(epoch)\n",
    "\n",
    "# Funkcja generująca próbki obrazów\n",
    "def sample_images(epoch):\n",
    "    noise = np.random.normal(0, 1, (25, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # Przeskalowanie do zakresu [0,1]\n",
    "    fig, axs = plt.subplots(5, 5)\n",
    "    count = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(gen_imgs[count, :, :, 0], cmap='gray')\n",
    "            axs[i, j].axis('off')\n",
    "            count += 1\n",
    "    plt.show()\n",
    "\n",
    "# Uruchomienie treningu\n",
    "train(epochs=10000, batch_size=64, save_interval=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b466c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
